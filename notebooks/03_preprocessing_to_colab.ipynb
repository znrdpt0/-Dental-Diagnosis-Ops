{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ULTIMATE AYARLAR ---\n",
    "IMG_SIZE = 1280  \n",
    "CLAHE_CLIP = 3.0 \n",
    "\n",
    "# Dosya YollarÄ±\n",
    "BASE_DIR = Path(\"../data\")\n",
    "RAW_DIR = BASE_DIR / \"raw/DENTEX/train/training_data/quadrant-enumeration-disease\"\n",
    "RAW_IMG_DIR = RAW_DIR / \"xrays\"\n",
    "RAW_JSON = RAW_DIR / \"train_quadrant_enumeration_disease.json\"\n",
    "\n",
    "# Ã‡Ä±ktÄ± Yeri\n",
    "OUTPUT_DIR = BASE_DIR / \"processed_ultimate\"\n",
    "\n",
    "# SÄ±nÄ±f HaritasÄ± (YOLO FormatÄ±)\n",
    "CLASS_MAPPING = {\n",
    "    0: 0, # Impacted\n",
    "    1: 1, # Caries\n",
    "    2: 2, # Periapical Lesion\n",
    "    3: 3  # Deep Caries\n",
    "}\n",
    "\n",
    "print(f\"Hedef Ã‡Ã¶zÃ¼nÃ¼rlÃ¼k: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Kontrast ArtÄ±rma (CLAHE): Aktif (Clip Limit: {CLAHE_CLIP})\")\n",
    "print(f\"Ham Veri Yolu: {RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clahe(image):\n",
    "    \"\"\"\n",
    "    RÃ¶ntgen gÃ¶rÃ¼ntÃ¼lerine Contrast Limited Adaptive Histogram Equalization uygular.\n",
    "    Bu, Ã§Ã¼rÃ¼klerin ve lezyonlarÄ±n daha belirgin olmasÄ±nÄ± saÄŸlar.\n",
    "    \"\"\"\n",
    "    # GÃ¶rÃ¼ntÃ¼ renkli (3 kanal) ise griye Ã§evir\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # CLAHE OluÅŸtur\n",
    "    clahe = cv2.createCLAHE(clipLimit=CLAHE_CLIP, tileGridSize=(8,8))\n",
    "    enhanced_img = clahe.apply(gray)\n",
    "    \n",
    "    # YOLO iÃ§in tekrar 3 kanallÄ± (BGR) formata Ã§evir\n",
    "    return cv2.cvtColor(enhanced_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def resize_letterbox(image, target_size):\n",
    "    \"\"\"\n",
    "    Resmi bozmadan target_size iÃ§ine sÄ±ÄŸdÄ±rÄ±r ve padding ekler.\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = min(target_size / w, target_size / h)\n",
    "    nw, nh = int(w * scale), int(h * scale)\n",
    "    \n",
    "    # Resmi kÃ¼Ã§Ã¼lt/bÃ¼yÃ¼t\n",
    "    image_resized = cv2.resize(image, (nw, nh))\n",
    "    \n",
    "    # Gri bir tuval oluÅŸtur\n",
    "    image_padded = np.full((target_size, target_size, 3), 114, dtype=np.uint8)\n",
    "    \n",
    "    # Ortala\n",
    "    dx = (target_size - nw) // 2\n",
    "    dy = (target_size - nh) // 2\n",
    "    image_padded[dy:dy+nh, dx:dx+nw] = image_resized\n",
    "    \n",
    "    return image_padded, scale, dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae510e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. KlasÃ¶r TemizliÄŸi\n",
    "if OUTPUT_DIR.exists():\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    (OUTPUT_DIR / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (OUTPUT_DIR / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. JSON YÃ¼kle ve BÃ¶l (Split)\n",
    "with open(RAW_JSON, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_images = data['images']\n",
    "random.seed(42) \n",
    "random.shuffle(all_images)\n",
    "\n",
    "split_idx = int(len(all_images) * 0.1)\n",
    "val_images = all_images[:split_idx]\n",
    "train_images = all_images[split_idx:]\n",
    "\n",
    "splits = {'train': train_images, 'val': val_images}\n",
    "\n",
    "ann_dict = {}\n",
    "for ann in data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    if img_id not in ann_dict: ann_dict[img_id] = []\n",
    "    ann_dict[img_id].append(ann)\n",
    "\n",
    "print(\"ðŸš€ Ä°ÅŸlem BaÅŸlÄ±yor...\")\n",
    "\n",
    "# 3. Ana DÃ¶ngÃ¼\n",
    "for split_name, images in splits.items():\n",
    "    print(f\"\\n--- {split_name.upper()} Seti Ä°ÅŸleniyor ({len(images)} resim) ---\")\n",
    "    \n",
    "    for img_info in tqdm(images):\n",
    "        file_name = img_info['file_name']\n",
    "        img_id = img_info['id']\n",
    "        src_path = RAW_IMG_DIR / file_name\n",
    "        \n",
    "        if not src_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # A. RESMÄ° OKU\n",
    "        img = cv2.imread(str(src_path))\n",
    "        \n",
    "        # B. CLAHE UYGULA (Parlat)\n",
    "        img_enhanced = apply_clahe(img)\n",
    "        \n",
    "        # C. RESIZE (1280px)\n",
    "        processed_img, scale, dx, dy = resize_letterbox(img_enhanced, IMG_SIZE)\n",
    "        \n",
    "        # D. KAYDET (Resim)\n",
    "        save_img_path = OUTPUT_DIR / split_name / 'images' / file_name\n",
    "        cv2.imwrite(str(save_img_path), processed_img)\n",
    "        \n",
    "        # E. ETÄ°KETLERÄ° DÃ–NÃœÅžTÃœR VE KAYDET\n",
    "        yolo_labels = []\n",
    "        if img_id in ann_dict:\n",
    "            for ann in ann_dict[img_id]:\n",
    "                cat_id = ann.get('category_id_3')\n",
    "                \n",
    "                if cat_id in CLASS_MAPPING:\n",
    "                    bbox = ann['bbox'] # x, y, w, h\n",
    "                    \n",
    "                    # Koordinat DÃ¶nÃ¼ÅŸÃ¼mÃ¼\n",
    "                    x = bbox[0] * scale + dx\n",
    "                    y = bbox[1] * scale + dy\n",
    "                    w = bbox[2] * scale\n",
    "                    h = bbox[3] * scale\n",
    "                    \n",
    "                    # YOLO Normalize (0-1 arasÄ±)\n",
    "                    xc = (x + w/2) / IMG_SIZE\n",
    "                    yc = (y + h/2) / IMG_SIZE\n",
    "                    wn = w / IMG_SIZE\n",
    "                    hn = h / IMG_SIZE\n",
    "                    \n",
    "                    # SÄ±nÄ±r KontrolÃ¼ (0-1 dÄ±ÅŸÄ±na taÅŸmasÄ±n)\n",
    "                    xc = max(0, min(1, xc))\n",
    "                    yc = max(0, min(1, yc))\n",
    "                    wn = max(0, min(1, wn))\n",
    "                    hn = max(0, min(1, hn))\n",
    "                    \n",
    "                    yolo_labels.append(f\"{CLASS_MAPPING[cat_id]} {xc:.6f} {yc:.6f} {wn:.6f} {hn:.6f}\")\n",
    "        \n",
    "        # Txt DosyasÄ±nÄ± Yaz\n",
    "        if yolo_labels:\n",
    "            label_name = Path(file_name).stem + \".txt\"\n",
    "            save_label_path = OUTPUT_DIR / split_name / 'labels' / label_name\n",
    "            with open(save_label_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "print(f\"\\nâœ… Ä°ÅŸlem TamamlandÄ±! Veriler burada: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "train: /content/dataset/processed_ultimate/train/images\n",
    "val: /content/dataset/processed_ultimate/val/images\n",
    "\n",
    "nc: 4\n",
    "names:\n",
    "  0: Impacted\n",
    "  1: Caries\n",
    "  2: Periapical Lesion\n",
    "  3: Deep Caries\n",
    "\"\"\"\n",
    "\n",
    "with open(OUTPUT_DIR / \"data.yaml\", 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"âœ… data.yaml oluÅŸturuldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../data && zip -r dental_ultimate.zip processed_ultimate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
